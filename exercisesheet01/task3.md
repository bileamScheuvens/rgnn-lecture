# Network Configuration
A hidden size of 3 leads to a hexagonal decision boundary.
The model thus has 3 layers, with 3 neurons each and a ReLU activation function after the first and second hidden layer.

# Theoretical Explanation
Each ReLU is essentially a linear boundary.
Since the model has 6 neurons with ReLU that and a hexagon is the best available approximation of a circle, it quickly finds this minimum.
